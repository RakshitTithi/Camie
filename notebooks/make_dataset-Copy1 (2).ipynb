{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "769581d3",
   "metadata": {},
   "source": [
    "# Generating density map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8768ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5133/389678396.py:9: DeprecationWarning: Please use `gaussian_filter` from the `scipy.ndimage` namespace, the `scipy.ndimage.filters` namespace is deprecated.\n",
      "  from scipy.ndimage.filters import gaussian_filter\n",
      "2022-11-26 13:02:54.387005: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-26 13:02:55.533811: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/tithi/.pyenv/versions/3.8.12/envs/ICYO/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2022-11-26 13:02:55.533868: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-11-26 13:02:55.635077: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-26 13:02:57.843485: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/tithi/.pyenv/versions/3.8.12/envs/ICYO/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2022-11-26 13:02:57.844376: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/tithi/.pyenv/versions/3.8.12/envs/ICYO/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2022-11-26 13:02:57.844395: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import h5py\n",
    "import scipy.io as io\n",
    "import PIL.Image as Image\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.ndimage.filters import gaussian_filter \n",
    "import scipy\n",
    "import json\n",
    "from matplotlib import cm as CM\n",
    "import tqdm\n",
    "import scipy\n",
    "import cv2\n",
    "import scipy.io as io \n",
    "from keras import backend as K\n",
    "from keras.models import model_from_json\n",
    "from keras.utils import load_img, img_to_array\n",
    "from matplotlib import cm as c\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Conv2D, Dense, Input, Flatten, MaxPooling2D, BatchNormalization, Activation, UpSampling2D\n",
    "from keras import optimizers\n",
    "from keras.optimizers import SGD\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.python.keras.initializers import RandomNormal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f49f2597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create density maps for images\n",
    "def gaussian_filter_density(gt):\n",
    "    print (gt.shape)\n",
    "    density = np.zeros(gt.shape, dtype=np.float32)\n",
    "    gt_count = np.count_nonzero(gt)\n",
    "    if gt_count == 0:\n",
    "        return density\n",
    "\n",
    "    pts = np.array(list(zip(np.nonzero(gt)[1], np.nonzero(gt)[0])))\n",
    "    leafsize = 2048\n",
    "    # build kdtree\n",
    "    tree = scipy.spatial.KDTree(pts.copy(), leafsize=leafsize)\n",
    "    # query kdtree\n",
    "    distances, locations = tree.query(pts, k=4)\n",
    "\n",
    "    print ('generate density...')\n",
    "    for i, pt in enumerate(pts):\n",
    "        pt2d = np.zeros(gt.shape, dtype=np.float32)\n",
    "        pt2d[pt[1],pt[0]] = 1.\n",
    "        if gt_count > 1:\n",
    "            sigma = (distances[i][1]+distances[i][2]+distances[i][3])*0.1\n",
    "        else:\n",
    "            sigma = np.average(np.array(gt.shape))/2./2. #case: 1 point\n",
    "        density += scipy.ndimage.filters.gaussian_filter(pt2d, sigma, mode='constant')\n",
    "    print ('done.')\n",
    "    return density"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56710371",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#Creating file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0581702e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "root='/home/tithi/code/RakshitTithi/ICYO/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c07d4d45",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "part_A_train=os.path.join(root,'raw_data/Shaghaitech_dataset/ShanghaiTech/part_A/train_data','images')\n",
    "part_A_test = os.path.join(root,'raw_data/Shaghaitech_dataset/ShanghaiTech/part_A/test_data','images')\n",
    "part_B_train = os.path.join(root,'raw_data/Shaghaitech_dataset/ShanghaiTech/part_B/train_data','images')\n",
    "part_B_test = os.path.join(root,'raw_data/Shaghaitech_dataset/ShanghaiTech/part_B/test_data','images')\n",
    "path_sets = [part_A_train,part_A_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c7e6bee",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/tithi/code/RakshitTithi/ICYO/raw_data/Shaghaitech_dataset/ShanghaiTech/part_A/train_data/images'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "part_A_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4f62d5",
   "metadata": {},
   "source": [
    "#create a list of absolute path of jpg file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05695044",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_paths=[]\n",
    "for path in path_sets:\n",
    "    for img_path in sorted(glob.glob(os.path.join(path,'*.jpg'))):\n",
    "        img_paths.append(img_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9839232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/tithi/code/RakshitTithi/ICYO/raw_data/Shaghaitech_dataset/ShanghaiTech/part_A/train_data/images/IMG_1.jpg',\n",
       " '/home/tithi/code/RakshitTithi/ICYO/raw_data/Shaghaitech_dataset/ShanghaiTech/part_A/train_data/images/IMG_10.jpg',\n",
       " '/home/tithi/code/RakshitTithi/ICYO/raw_data/Shaghaitech_dataset/ShanghaiTech/part_A/train_data/images/IMG_100.jpg',\n",
       " '/home/tithi/code/RakshitTithi/ICYO/raw_data/Shaghaitech_dataset/ShanghaiTech/part_A/train_data/images/IMG_101.jpg',\n",
       " '/home/tithi/code/RakshitTithi/ICYO/raw_data/Shaghaitech_dataset/ShanghaiTech/part_A/train_data/images/IMG_102.jpg',\n",
       " '/home/tithi/code/RakshitTithi/ICYO/raw_data/Shaghaitech_dataset/ShanghaiTech/part_A/train_data/images/IMG_103.jpg',\n",
       " '/home/tithi/code/RakshitTithi/ICYO/raw_data/Shaghaitech_dataset/ShanghaiTech/part_A/train_data/images/IMG_104.jpg',\n",
       " '/home/tithi/code/RakshitTithi/ICYO/raw_data/Shaghaitech_dataset/ShanghaiTech/part_A/train_data/images/IMG_105.jpg',\n",
       " '/home/tithi/code/RakshitTithi/ICYO/raw_data/Shaghaitech_dataset/ShanghaiTech/part_A/train_data/images/IMG_106.jpg',\n",
       " '/home/tithi/code/RakshitTithi/ICYO/raw_data/Shaghaitech_dataset/ShanghaiTech/part_A/train_data/images/IMG_107.jpg',\n",
       " '/home/tithi/code/RakshitTithi/ICYO/raw_data/Shaghaitech_dataset/ShanghaiTech/part_A/train_data/images/IMG_108.jpg',\n",
       " '/home/tithi/code/RakshitTithi/ICYO/raw_data/Shaghaitech_dataset/ShanghaiTech/part_A/train_data/images/IMG_109.jpg',\n",
       " '/home/tithi/code/RakshitTithi/ICYO/raw_data/Shaghaitech_dataset/ShanghaiTech/part_A/train_data/images/IMG_11.jpg',\n",
       " '/home/tithi/code/RakshitTithi/ICYO/raw_data/Shaghaitech_dataset/ShanghaiTech/part_A/train_data/images/IMG_110.jpg',\n",
       " '/home/tithi/code/RakshitTithi/ICYO/raw_data/Shaghaitech_dataset/ShanghaiTech/part_A/train_data/images/IMG_111.jpg',\n",
       " '/home/tithi/code/RakshitTithi/ICYO/raw_data/Shaghaitech_dataset/ShanghaiTech/part_A/train_data/images/IMG_112.jpg',\n",
       " '/home/tithi/code/RakshitTithi/ICYO/raw_data/Shaghaitech_dataset/ShanghaiTech/part_A/train_data/images/IMG_113.jpg',\n",
       " '/home/tithi/code/RakshitTithi/ICYO/raw_data/Shaghaitech_dataset/ShanghaiTech/part_A/train_data/images/IMG_114.jpg',\n",
       " '/home/tithi/code/RakshitTithi/ICYO/raw_data/Shaghaitech_dataset/ShanghaiTech/part_A/train_data/images/IMG_115.jpg',\n",
       " '/home/tithi/code/RakshitTithi/ICYO/raw_data/Shaghaitech_dataset/ShanghaiTech/part_A/train_data/images/IMG_116.jpg',\n",
       " '/home/tithi/code/RakshitTithi/ICYO/raw_data/Shaghaitech_dataset/ShanghaiTech/part_A/train_data/images/IMG_117.jpg',\n",
       " '/home/tithi/code/RakshitTithi/ICYO/raw_data/Shaghaitech_dataset/ShanghaiTech/part_A/train_data/images/IMG_118.jpg',\n",
       " '/home/tithi/code/RakshitTithi/ICYO/raw_data/Shaghaitech_dataset/ShanghaiTech/part_A/train_data/images/IMG_119.jpg',\n",
       " '/home/tithi/code/RakshitTithi/ICYO/raw_data/Shaghaitech_dataset/ShanghaiTech/part_A/train_data/images/IMG_12.jpg',\n",
       " '/home/tithi/code/RakshitTithi/ICYO/raw_data/Shaghaitech_dataset/ShanghaiTech/part_A/train_data/images/IMG_120.jpg']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_paths=img_paths[:25]\n",
    "img_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca21a73e",
   "metadata": {},
   "source": [
    "#creating density map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2363e1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                         | 0/25 [00:00<?, ?it/s]/tmp/ipykernel_5133/4212836471.py:24: DeprecationWarning: Please use `gaussian_filter` from the `scipy.ndimage` namespace, the `scipy.ndimage.filters` namespace is deprecated.\n",
      "  density += scipy.ndimage.filters.gaussian_filter(pt2d, sigma, mode='constant')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 1024)\n",
      "generate density...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|███▉                                                                                             | 1/25 [00:42<16:58, 42.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "(768, 1024)\n",
      "generate density...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|███████▊                                                                                         | 2/25 [01:22<15:45, 41.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "(768, 1024)\n",
      "generate density...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|███████████▋                                                                                     | 3/25 [01:40<11:14, 30.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "(768, 1024)\n",
      "generate density...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|███████████████▌                                                                                 | 4/25 [01:57<08:47, 25.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "(768, 1024)\n",
      "generate density...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████▍                                                                             | 5/25 [02:10<06:53, 20.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "(768, 1024)\n",
      "generate density...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|███████████████████████▎                                                                         | 6/25 [02:15<04:52, 15.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "(768, 1024)\n",
      "generate density...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|███████████████████████████▏                                                                     | 7/25 [02:29<04:26, 14.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "(768, 1024)\n",
      "generate density...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███████████████████████████████                                                                  | 8/25 [02:44<04:15, 15.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "(768, 1024)\n",
      "generate density...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|██████████████████████████████████▉                                                              | 9/25 [03:15<05:22, 20.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "(768, 1024)\n",
      "generate density...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|██████████████████████████████████████▍                                                         | 10/25 [03:24<04:10, 16.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "(768, 1024)\n",
      "generate density...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|██████████████████████████████████████████▏                                                     | 11/25 [04:05<05:36, 24.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "(768, 1024)\n",
      "generate density...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|██████████████████████████████████████████████                                                  | 12/25 [04:37<05:44, 26.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "(768, 1024)\n",
      "generate density...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████████████████████████████████████████████████▉                                              | 13/25 [04:49<04:25, 22.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "(768, 1024)\n",
      "generate density...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████████████████████████████████████████████████████▊                                          | 14/25 [05:04<03:40, 20.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "(768, 1024)\n",
      "generate density...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████████████████████████████████████▌                                      | 15/25 [05:48<04:31, 27.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "(768, 1024)\n",
      "generate density...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|█████████████████████████████████████████████████████████████▍                                  | 16/25 [07:03<06:14, 41.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "(768, 1024)\n",
      "generate density...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|█████████████████████████████████████████████████████████████████▎                              | 17/25 [07:26<04:47, 35.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "(768, 1024)\n",
      "generate density...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|█████████████████████████████████████████████████████████████████████                           | 18/25 [07:40<03:25, 29.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "(768, 1024)\n",
      "generate density...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|████████████████████████████████████████████████████████████████████████▉                       | 19/25 [08:01<02:40, 26.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "(768, 1024)\n",
      "generate density...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████████████████████████████████████████████████████████████████████████▊                   | 20/25 [08:32<02:20, 28.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "(768, 1024)\n",
      "generate density...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████████████████████████████████████████████████████████████████████████████▋               | 21/25 [08:55<01:46, 26.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "(768, 1024)\n",
      "generate density...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████████████████████████████████████████████████████████████████████████████████▍           | 22/25 [10:09<02:02, 40.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "(768, 1024)\n",
      "generate density...\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "i = 0\n",
    "for img_path in tqdm(img_paths):\n",
    "        \n",
    "    \n",
    "    # Load sparse matrix\n",
    "    mat = io.loadmat(img_path.replace('.jpg','.mat').replace('images','ground-truth').replace('IMG_','GT_IMG_'))\n",
    "    #Read image\n",
    "    img= plt.imread(img_paths[0])\n",
    "\n",
    "    # Create a zero matrix of image size\n",
    "    k = np.zeros((img.shape[0],img.shape[1]))\n",
    "\n",
    "    gt = mat[\"image_info\"][0,0][0,0][0]\n",
    "\n",
    "    #Generate hot encoded matrix of sparse matrix\n",
    "    for i in range(0,len(gt)):\n",
    "        if int(gt[i][1])<img.shape[0] and int(gt[i][0])<img.shape[1]:\n",
    "            k[int(gt[i][1]),int(gt[i][0])]=1\n",
    "\n",
    "    # generate density map\n",
    "    k = gaussian_filter_density(k)\n",
    "\n",
    "    # File path to save density map\n",
    "    file_path = img_path.replace('.jpg','.h5').replace('images','ground-truth')\n",
    "\n",
    "\n",
    "    with h5py.File(file_path, 'w') as hf:\n",
    "            hf['density'] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c41f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = img_paths[0].replace('.jpg','.h5').replace('images','ground-truth') \n",
    "print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cd03cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample Ground Truth\n",
    "gt_file = h5py.File(file_path,'r')\n",
    "groundtruth = np.asarray(gt_file['density'])\n",
    "plt.imshow(groundtruth,cmap=CM.jet)\n",
    "Sum=np.sum(groundtruth)\n",
    "print(\"Sum = \" ,Sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b96b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(file_path.replace('.h5','.jpg').replace('ground-truth','images'))\n",
    "plt.imshow(img)\n",
    "print(file_path.replace('.h5','.jpg').replace('ground-truth','images'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048c659e",
   "metadata": {},
   "source": [
    "#Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18a4f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "root='/home/tithi/code/RakshitTithi/ICYO/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91145c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "part_A_train=os.path.join(root,'raw_data/Shaghaitech_dataset/ShanghaiTech/part_A/train_data','images')\n",
    "part_A_test = os.path.join(root,'raw_data/Shaghaitech_dataset/ShanghaiTech/part_A/test_data','images')\n",
    "part_B_train = os.path.join(root,'raw_data/Shaghaitech_dataset/ShanghaiTech/part_B/train_data','images')\n",
    "part_B_test = os.path.join(root,'raw_data/Shaghaitech_dataset/ShanghaiTech/part_B/test_data','images')\n",
    "temp = 'test_images'\n",
    "path_sets = [part_A_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb2a2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_paths = []\n",
    "\n",
    "for path in path_sets:\n",
    "    \n",
    "    for img_path in glob.glob(os.path.join(path, '*.jpg')):\n",
    "        \n",
    "        img_paths.append(str(img_path))\n",
    "        \n",
    "print(\"Total images : \",len(img_paths))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701b5ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_img(path):\n",
    "    #Function to load,normalize and return image \n",
    "    im = Image.open(path).convert('RGB')\n",
    "    \n",
    "    im = np.array(im)\n",
    "    \n",
    "    im = im/255.0\n",
    "    \n",
    "    im[:,:,0]=(im[:,:,0]-0.485)/0.229\n",
    "    im[:,:,1]=(im[:,:,1]-0.456)/0.224\n",
    "    im[:,:,2]=(im[:,:,2]-0.406)/0.225\n",
    "\n",
    "    #print(im.shape)\n",
    "    #im = np.expand_dims(im,axis  = 0)\n",
    "    return im\n",
    "\n",
    "def get_input(path):\n",
    "    path = path[0] \n",
    "    img = create_img(path)\n",
    "    return(img)\n",
    "\n",
    "def get_output(path):\n",
    "    #import target\n",
    "    #resize target\n",
    "    \n",
    "    gt_file = h5py.File(path,'r')\n",
    "    \n",
    "    target = np.asarray(gt_file['density'])\n",
    "    \n",
    "    img = cv2.resize(target,(int(target.shape[1]/8),int(target.shape[0]/8)),interpolation = cv2.INTER_CUBIC)*64\n",
    "    \n",
    "    img = np.expand_dims(img,axis  = 3)\n",
    "    \n",
    "    #print(img.shape)\n",
    "    \n",
    "    return img\n",
    "    \n",
    "    \n",
    "    \n",
    "def preprocess_input(image,target):\n",
    "    #crop image\n",
    "    #crop target\n",
    "    #resize target\n",
    "    crop_size = (int(image.shape[0]/2),int(image.shape[1]/2))\n",
    "    \n",
    "    \n",
    "    if random.randint(0,9)<= -1:            \n",
    "            dx = int(random.randint(0,1)*image.shape[0]*1./2)\n",
    "            dy = int(random.randint(0,1)*image.shape[1]*1./2)\n",
    "    else:\n",
    "            dx = int(random.random()*image.shape[0]*1./2)\n",
    "            dy = int(random.random()*image.shape[1]*1./2)\n",
    "\n",
    "    #print(crop_size , dx , dy)\n",
    "    img = image[dx : crop_size[0]+dx , dy:crop_size[1]+dy]\n",
    "    \n",
    "    target_aug = target[dx:crop_size[0]+dx,dy:crop_size[1]+dy]\n",
    "    #print(img.shape)\n",
    "\n",
    "    return(img,target_aug)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73267fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image data generator \n",
    "def image_generator(files, batch_size = 64):\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        input_path = np.random.choice(a = files, size = batch_size)\n",
    "        \n",
    "        batch_input = []\n",
    "        batch_output = [] \n",
    "          \n",
    "#         for input_path in batch_paths:\n",
    "\n",
    "        inputt = get_input(input_path )\n",
    "        output = get_output(input_path[0].replace('.jpg','.h5').replace('images','ground-truth') )\n",
    "\n",
    "\n",
    "        batch_input += [inputt]\n",
    "        batch_output += [output]\n",
    "\n",
    "\n",
    "        batch_x = np.array( batch_input )\n",
    "        batch_y = np.array( batch_output )\n",
    "\n",
    "        yield( batch_x, batch_y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ba712b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_mod(model , str1 , str2):\n",
    "    model.save_weights(str1)\n",
    "    \n",
    "    model_json = model.to_json()\n",
    "    \n",
    "    with open(str2, \"w\") as json_file:\n",
    "        json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8b23f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights_vgg(model):\n",
    "    #vgg =  VGG16(weights='imagenet', include_top=False)\n",
    "    \n",
    "    json_file = open('models/VGG_16.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    loaded_model.load_weights(\"weights/VGG_16.h5\")\n",
    "      \n",
    "    \n",
    "    vgg = loaded_model\n",
    "    \n",
    "    vgg_weights=[]                         \n",
    "    for layer in vgg.layers:\n",
    "        if('conv' in layer.name):\n",
    "            vgg_weights.append(layer.get_weights())\n",
    "    \n",
    "    \n",
    "    offset=0\n",
    "    i=0\n",
    "    while(i<10):\n",
    "        if('conv' in model.layers[i+offset].name):\n",
    "            model.layers[i+offset].set_weights(vgg_weights[i])\n",
    "            i=i+1\n",
    "            #print('h')\n",
    "            \n",
    "        else:\n",
    "            offset=offset+1\n",
    "\n",
    "    return (model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6eda3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance_loss(y_true, y_pred):\n",
    "    # Euclidean distance as a measure of loss (Loss function) \n",
    "    return K.sqrt(K.sum(K.square(y_pred - y_true), axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af7610e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network model : VGG + Conv\n",
    "def CrowdNet():  \n",
    "            #Variable Input Size\n",
    "            rows = None\n",
    "            cols = None\n",
    "            \n",
    "            #Batch Normalisation option\n",
    "            \n",
    "            batch_norm = 0\n",
    "            kernel = (3, 3)\n",
    "            init = RandomNormal(stddev=0.01)\n",
    "            model = Sequential() \n",
    "            \n",
    "            #custom VGG:\n",
    "            \n",
    "            if(batch_norm):\n",
    "                model.add(Conv2D(64, kernel_size = kernel, input_shape = (rows,cols,3),activation = 'relu', padding='same'))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(Conv2D(64, kernel_size = kernel,activation = 'relu', padding='same'))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(MaxPooling2D(strides=2))\n",
    "                model.add(Conv2D(128,kernel_size = kernel, activation = 'relu', padding='same'))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(Conv2D(128,kernel_size = kernel, activation = 'relu', padding='same'))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(MaxPooling2D(strides=2))\n",
    "                model.add(Conv2D(256,kernel_size = kernel, activation = 'relu', padding='same'))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(Conv2D(256,kernel_size = kernel, activation = 'relu', padding='same'))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(Conv2D(256,kernel_size = kernel, activation = 'relu', padding='same'))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(MaxPooling2D(strides=2))            \n",
    "                model.add(Conv2D(512, kernel_size = kernel,activation = 'relu', padding='same'))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(Conv2D(512, kernel_size = kernel,activation = 'relu', padding='same'))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(Conv2D(512, kernel_size = kernel,activation = 'relu', padding='same'))\n",
    "                model.add(BatchNormalization())\n",
    "                \n",
    "            else:\n",
    "                model.add(Conv2D(64, kernel_size = kernel,activation = 'relu', padding='same',input_shape = (rows, cols, 3), kernel_initializer = init))\n",
    "                model.add(Conv2D(64, kernel_size = kernel,activation = 'relu', padding='same', kernel_initializer = init))\n",
    "                model.add(MaxPooling2D(strides=2))\n",
    "                model.add(Conv2D(128,kernel_size = kernel, activation = 'relu', padding='same', kernel_initializer = init))\n",
    "                model.add(Conv2D(128,kernel_size = kernel, activation = 'relu', padding='same', kernel_initializer = init))\n",
    "                model.add(MaxPooling2D(strides=2))\n",
    "                model.add(Conv2D(256,kernel_size = kernel, activation = 'relu', padding='same', kernel_initializer = init))\n",
    "                model.add(Conv2D(256,kernel_size = kernel, activation = 'relu', padding='same', kernel_initializer = init))\n",
    "                model.add(Conv2D(256,kernel_size = kernel, activation = 'relu', padding='same', kernel_initializer = init))\n",
    "                model.add(MaxPooling2D(strides=2))            \n",
    "                model.add(Conv2D(512, kernel_size = kernel,activation = 'relu', padding='same', kernel_initializer = init))\n",
    "                model.add(Conv2D(512, kernel_size = kernel,activation = 'relu', padding='same', kernel_initializer = init))\n",
    "                model.add(Conv2D(512, kernel_size = kernel,activation = 'relu', padding='same', kernel_initializer = init))\n",
    "                \n",
    "                \n",
    "\n",
    "                \n",
    "            #Conv2D\n",
    "            model.add(Conv2D(512, (3, 3), activation='relu', dilation_rate = 2, kernel_initializer = init, padding = 'same'))\n",
    "            model.add(Conv2D(512, (3, 3), activation='relu', dilation_rate = 2, kernel_initializer = init, padding = 'same'))\n",
    "            model.add(Conv2D(512, (3, 3), activation='relu', dilation_rate = 2, kernel_initializer = init, padding = 'same'))\n",
    "            model.add(Conv2D(256, (3, 3), activation='relu', dilation_rate = 2, kernel_initializer = init, padding = 'same'))\n",
    "            model.add(Conv2D(128, (3, 3), activation='relu', dilation_rate = 2, kernel_initializer = init, padding = 'same'))\n",
    "            model.add(Conv2D(64, (3, 3), activation='relu', dilation_rate = 2, kernel_initializer = init, padding = 'same'))\n",
    "            model.add(Conv2D(1, (1, 1), activation='relu', dilation_rate = 1, kernel_initializer = init, padding = 'same'))\n",
    "        \n",
    "            sgd = SGD(lr = 1e-7, decay = (5*1e-4), momentum = 0.95)\n",
    "            model.compile(optimizer=sgd, loss=euclidean_distance_loss, metrics=['mse'])\n",
    "            \n",
    "            model = init_weights_vgg(model)\n",
    "            \n",
    "            return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c8f92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CrowdNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fd67a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63594f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path_train=img_paths[:25]\n",
    "len(img_path_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b481bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = image_generator(img_path_train,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2a4151",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGD(lr = 1e-7, decay = (5*1e-4), momentum = 0.95)\n",
    "model.compile(optimizer=sgd, loss=euclidean_distance_loss, metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cee231",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit(train_gen,epochs=1,steps_per_epoch= 700 , verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfa5cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_mod(model,\"weights/model_A_weights.h5\",\"models/Model.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15facb04",
   "metadata": {},
   "source": [
    "#Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473ff937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    # Function to load and return neural network model \n",
    "    json_file = open('models/Model.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    loaded_model.load_weights(\"weights/model_A_weights.h5\")\n",
    "    return loaded_model\n",
    "\n",
    "def create_img(path):\n",
    "    #Function to load,normalize and return image \n",
    "    print(path)\n",
    "    im = Image.open(path).convert('RGB')\n",
    "    \n",
    "    im = np.array(im)\n",
    "    \n",
    "    im = im/255.0\n",
    "    \n",
    "    im[:,:,0]=(im[:,:,0]-0.485)/0.229\n",
    "    im[:,:,1]=(im[:,:,1]-0.456)/0.224\n",
    "    im[:,:,2]=(im[:,:,2]-0.406)/0.225\n",
    "\n",
    "\n",
    "    im = np.expand_dims(im,axis  = 0)\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b26b548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(path):\n",
    "    #Function to load image,predict heat map, generate count and return (count , image , heat map)\n",
    "    model = load_model()\n",
    "    image = create_img(path)\n",
    "    ans = model.predict(image)\n",
    "    count = np.sum(ans)\n",
    "    return count,image,ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc7c9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb299f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans,img,hmap = predict('raw_data/Shaghaitech_dataset/ShanghaiTech/part_A/test_data/images/IMG_131.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10f76e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ans)\n",
    "#Print count, image, heat map\n",
    "plt.imshow(img.reshape(img.shape[1],img.shape[2],img.shape[3]))\n",
    "plt.show()\n",
    "plt.imshow(hmap.reshape(hmap.shape[1],hmap.shape[2]) , cmap = c.jet )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35885bd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1c8d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4dff05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = h5py.File('raw_data/Shaghaitech_dataset/ShanghaiTech/part_A/test_data/images/IMG_131.jpg' , 'r')\n",
    "temp =plt.imshow('raw_data/Shaghaitech_dataset/ShanghaiTech/part_A/test_data/images/IMG_131.jpg' )\n",
    "temp_1 = np.asarray(temp['density'])\n",
    "#plt.imshow(temp_1,cmap = c.jet)\n",
    "print(\"Original Count : \",int(np.sum(temp_1)) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546e771c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

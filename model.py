# -*- coding: utf-8 -*-
"""model.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lhlFgan8QCNL3aAHpPvZikTx7fimK9kW
"""

from keras.models import Model, Sequential
from keras.layers import Conv2D, Dense, Input, Flatten, MaxPool2D
from keras import optimizers
from keras.applications.vgg16 import VGG16, preprocess_input
from keras.callbacks import ModelCheckpoint, EarlyStopping

#baseline model architecture- 16 layers, without padding, the same filter size, 5 max pooling layers
base_model = Sequential()
#13 conv layers 
base_model.add(Conv2D(input_shape=(224,224,3),filters=64,kernel_size=(3,3), activation="relu"))
base_model.add(Conv2D(filters=64,kernel_size=(3,3), activation="relu"))
base_model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))
base_model.add(Conv2D(filters=64,kernel_size=(3,3), activation="relu"))
base_model.add(Conv2D(filters=64,kernel_size=(3,3), activation="relu"))
base_model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))
base_model.add(Conv2D(filters=64,kernel_size=(3,3), activation="relu"))
base_model.add(Conv2D(filters=64,kernel_size=(3,3), activation="relu"))
base_model.add(Conv2D(filters=64,kernel_size=(3,3), activation="relu"))
base_model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))
base_model.add(Conv2D(filters=64,kernel_size=(3,3), activation="relu"))
base_model.add(Conv2D(filters=64,kernel_size=(3,3), activation="relu"))
base_model.add(Conv2D(filters=64,kernel_size=(3,3), activation="relu"))
base_model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))
base_model.add(Conv2D(filters=64,kernel_size=(3,3), activation="relu"))
base_model.add(Conv2D(filters=64,kernel_size=(3,3), activation="relu"))
base_model.add(Conv2D(filters=64,kernel_size=(3,3), activation="relu"))
#flatten the layer
base_model.add(Flatten())
#3 dense layers
base_model.add(Dense(units=4096,activation="relu"))
base_model.add(Dense(units=4096,activation="relu"))
base_model.add(Dense(units=2, activation="sigmoid"))
#compile the model
base_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
#fit(train) the model
#print("Fit baseline model on training data")
# base_hist = model.fit(X, y,
#           batch_size=16, # When batch size is too small --> no generalization
#           epochs=100,    # When batch size is too large --> slow computations
#           validation_split=0.3,
#           verbose=0)

base_model.summary()

#CNN Model with transferred learning from VGG16
def build_model():
  #instantiate the model
  vgg_16 = VGG16(weights='imagenet', include_top=False) 
  #Create your own input format (here 3x200x200)
  input = Input(shape=(3,200,200),name = 'image_input')
  #Use the generated model 
  output_vgg16 = vgg_16(input)
  #Add the fully-connected layers 
  x = Flatten(name='flatten')(output_vgg16)
  x = Dense(4096, activation='relu', name='fc1')(x)
  x = Dense(4096, activation='relu', name='fc2')(x)
  x = Dense(2, activation='sigmoid', name='predictions')(x)

  #Create your own model 
  model = Model(input=input, output=x)
  
  #checkpoint to save the model when the current epoch validation loss is lesser than the previous epoch.
  file_path =  "blah"
  checkpoint = ModelCheckpoint(filepath=file_path,
                              monitor='val_loss',
                              verbose =1,
                              save_best_only=False,
                              save_weights_only=False,
                              mode='auto',
                              save_freq='epoch'
                              )
  #Early stopping
  es = EarlyStopping(patience=40,
                    verbose=1,
                    monitor='val_accuracy')
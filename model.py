# -*- coding: utf-8 -*-
"""model

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lhlFgan8QCNL3aAHpPvZikTx7fimK9kW
"""

import os, glob
import scipy
import scipy.io as io 
import numpy as np
import h5py
from matplotlib import pyplot as plt
from google.colab import drive
from keras.models import Model, Sequential
from keras.layers import Conv2D, Dense, Input, Flatten, MaxPool2D, BatchNormalization, Activation, UpSampling2D
from keras import optimizers
from keras.applications.vgg16 import VGG16, preprocess_input
from keras.callbacks import ModelCheckpoint, EarlyStopping
from tensorflow.python.keras.initializers import RandomNormal

#baseline model architecture- 16 layers, without padding, the same filter size, 5 max pooling layers
base_model = Sequential()
#13 conv layers 
base_model.add(Conv2D(input_shape=(224,224,3),filters=64,kernel_size=(3,3), activation="relu"))
base_model.add(Conv2D(filters=64,kernel_size=(3,3), activation="relu"))
base_model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))
base_model.add(Conv2D(filters=64,kernel_size=(3,3), activation="relu"))
base_model.add(Conv2D(filters=64,kernel_size=(3,3), activation="relu"))
base_model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))
base_model.add(Conv2D(filters=64,kernel_size=(3,3), activation="relu"))
base_model.add(Conv2D(filters=64,kernel_size=(3,3), activation="relu"))
base_model.add(Conv2D(filters=64,kernel_size=(3,3), activation="relu"))
base_model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))
base_model.add(Conv2D(filters=64,kernel_size=(3,3), activation="relu"))
base_model.add(Conv2D(filters=64,kernel_size=(3,3), activation="relu"))
base_model.add(Conv2D(filters=64,kernel_size=(3,3), activation="relu"))
base_model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))
base_model.add(Conv2D(filters=64,kernel_size=(3,3), activation="relu"))
base_model.add(Conv2D(filters=64,kernel_size=(3,3), activation="relu"))
base_model.add(Conv2D(filters=64,kernel_size=(3,3), activation="relu"))
#flatten the layer
base_model.add(Flatten())
#3 dense layers
base_model.add(Dense(units=4096,activation="relu"))
base_model.add(Dense(units=4096,activation="relu"))
base_model.add(Dense(units=2, activation="sigmoid"))
#compile the model
base_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
#fit(train) the model
#print("Fit baseline model on training data")
# base_hist = model.fit(X, y,
#           batch_size=16, # When batch size is too small --> no generalization
#           epochs=100,    # When batch size is too large --> slow computations
#           validation_split=0.3,
#           verbose=0)

base_model.summary()

root='/content/drive/MyDrive/Colab Notebooks/ICYO/Shaghaitech dataset/ShanghaiTech'

part_A_train=os.path.join(root,'part_A/train_data','images')
part_A_test = os.path.join(root,'part_A/test_data','images')
part_B_train = os.path.join(root,'part_B/train_data','images')
part_B_test = os.path.join(root,'part_B/test_data','images')
path_sets = [part_A_train,part_A_test]

from google.colab import drive
drive.mount('/content/drive')

root

path_sets

img_paths=[]
for path in path_sets:
    for img_path in sorted(glob.glob(os.path.join(path,'*.jpg'))):
        img_paths.append(img_path)

test_path = img_paths[1]
test_path

# img_paths

def gaussian_filter_density(gt):
    print (gt.shape)
    density = np.zeros(gt.shape, dtype=np.float32)
    gt_count = np.count_nonzero(gt)
    if gt_count == 0:
        return density

    pts = np.array(list(zip(np.nonzero(gt)[1], np.nonzero(gt)[0])))
    leafsize = 2048
    # build kdtree
    tree = scipy.spatial.KDTree(pts.copy(), leafsize=leafsize)
    # query kdtree
    distances, locations = tree.query(pts, k=4)

    print ('generate density...')
    for i, pt in enumerate(pts):
        pt2d = np.zeros(gt.shape, dtype=np.float32)
        pt2d[pt[1],pt[0]] = 1.
        if gt_count > 1:
            sigma = (distances[i][1]+distances[i][2]+distances[i][3])*0.1
        else:
            sigma = np.average(np.array(gt.shape))/2./2. #case: 1 point
        density += scipy.ndimage.filters.gaussian_filter(pt2d, sigma, mode='constant')
    print ('done.')
    return density

# for img_path in img_paths:
# #    print(img_path)
#     #loading mat files and modifying the names as per the corresponding images
mat=io.loadmat(test_path.replace('.jpg','.mat').replace('images','ground-truth').replace('IMG_','GT_IMG_'))
img=plt.imread(test_path) #reading the image as a tensor
k = np.zeros((img.shape[0],img.shape[1])) #creating an array of 0s
gt=mat['image_info'][0,0][0,0][0] #accessing ground truth elements
for i in range(0,len(gt)):
    if int(gt[i][1])<img.shape[0] and int(gt[i][0])<img.shape[1]:
        k[int(gt[i][1]),int(gt[i][0])]=1
k = gaussian_filter_density(k)
with h5py.File(img_path.replace('.jpg','.h5').replace('images','ground-truth'), 'w') as hf:
        hf['density'] = k



# hf['density']

h5py.File(hf, 'r')

density = np.zeros(gt.shape, dtype=np.float32)

gt_count = np.count_nonzero(gt)
gt_count

# if gt_count == 0:
#   return density

pts = np.array(list(zip(np.nonzero(gt)[1], np.nonzero(gt)[0])))
pts

pts.shape

h5_root = '/content/drive/MyDrive/Colab Notebooks/ICYO/Shaghaitech dataset/shanghaitech_h5_empty/ShanghaiTech'

h5_part_A_train = os.path.join(h5_root,'part_A/train_data','images')

h5_test = '/content/drive/MyDrive/Colab Notebooks/ICYO/Shaghaitech dataset/shanghaitech_h5_empty/ShanghaiTech/part_A/train_data/images/IMG_1.jpg'

# # file_path = img_paths[0].replace('.jpg','.h5').replace('images','ground') 
# # print(file_path)
# type(file_path)

gt_file = h5py.File(file_path,'r')
groundtruth = np.asarray(gt_file['density'])
plt.imshow(groundtruth,cmap=CM.jet)
print("Sum = " ,np.sum(groundtruth))

#CNN Model with transferred learning from VGG16
# def build_model():
#instantiate the model
vgg_16 = VGG16(weights='imagenet', include_top=False) 
x = vgg_16.get_layer('block4_conv3').output
#Create your own input format (here 3x200x200)
# input = Input(shape=(3,200,200),name = 'image_input')
# #Use the generated model 
# output_vgg16 = vgg_16(input)
x = BatchNormalization()(x)
#Upsampling to convolve the dense layer with Kernel + conv2D layers
x = UpSampling2D(size=(8, 8))(x)
x = Conv2D(filters=512, kernel_size=(3, 3), dilation_rate=2, padding='same', use_bias=False, kernel_initializer=RandomNormal(stddev=0.01))(x)
x = BatchNormalization()(x)
x = Activation('relu')(x)
x = Conv2D(filters=512, kernel_size=(3, 3), dilation_rate=2, padding='same', use_bias=False, kernel_initializer=RandomNormal(stddev=0.01))(x)
x = BatchNormalization()(x)
x = Activation('relu')(x)
x = Conv2D(filters=512, kernel_size=(3, 3), dilation_rate=2, padding='same', use_bias=False, kernel_initializer=RandomNormal(stddev=0.01))(x)
x = BatchNormalization()(x)
x = Activation('relu')(x)
x = Conv2D(filters=256, kernel_size=(3, 3), dilation_rate=2, padding='same', use_bias=False, kernel_initializer=RandomNormal(stddev=0.01))(x)
x = BatchNormalization()(x)
x = Activation('relu')(x)
x = Conv2D(filters=128, kernel_size=(3, 3), dilation_rate=2, padding='same', use_bias=False, kernel_initializer=RandomNormal(stddev=0.01))(x)
x = BatchNormalization()(x)
x = Activation('relu')(x)
x = Conv2D(filters=64, kernel_size=(3, 3), dilation_rate=2, padding='same', use_bias=False, kernel_initializer=RandomNormal(stddev=0.01))(x)
x = BatchNormalization()(x)
x = Activation('relu')(x)
x = Conv2D(filters=1, kernel_size=(1, 1), dilation_rate=1, padding='same', use_bias=True, kernel_initializer=RandomNormal(stddev=0.01))(x)
x = Activation('linear')(x)
#Create your own model 
vgg_16_model = Model(inputs=vgg_16.input, outputs=x)
# Classification with 2 classes
vgg_16_model.compile(loss='mse', 
              optimizer='adam',
              metrics=['mae'])

# #checkpoint to save the model when the current epoch validation loss is lesser than the previous epoch.
# file_path =  "blah"
# checkpoint = ModelCheckpoint(filepath=file_path,
#                             monitor='val_loss',
#                             verbose =1,
#                             save_best_only=False,
#                             save_weights_only=False,
#                             mode='auto',
#                             save_freq='epoch'
#                             )
#Early stopping
es = EarlyStopping(patience=40,
                  verbose=1,
                  monitor='val_accuracy')
vgg_16_model.fit(img, gt)
          # verbose=1)


# vgg_16_model.summary()